[{"authors":["chhh"],"categories":null,"content":"Best way to describe myself would be: a software engineer with a lot of hands on experience in mass-spectrometry (liquid and gas chromatography, ion mobility).\nUsual course of action - acquire new data, explore it with visualizations, come up with a suitable processing algorithm. In most cases this results in desktop GUI applications for the viz part and CLI programs to perform batch data processing.\n Prefer writing code in Java or C#, with occasional forays into C++. Contributing to Open Source projects like Apache NetBeans . Sharing love through Maven Central. Daily drivers: Java, Gradle, Maven, C# (NET Standard 2.0), Git, SQL Occasional usage: R, Python, Nodejs, React, Hugo, Gatsby and much more  Some project links:\n  BatMass (Java, NetBeans Platform) - Mass spectrometry data visualization and processing.  MSFTBX (Java, gRPC, JNA) - Java library for unified mass-spec data access: mzML, mzXML, pepXML, protXML, mzId, mzIdentML, etc. Available through Maven Central.  FragPipe (Java Swing, JNA) - Pipelined complete LC/MS proteomic analysis with a graphical interface. Peptide/protein search, validation, FDR estimation, quantitation, PTM analysis, spectral library building etc.  MSFragger (Java) - Ultra-fast proteomics search engine for Closed and Open searches.  IMTBX+Grppr (C#, WinForms, Java) - A suite of tools for extracting signals from Ion Mobility (IM) data (IM-MS, LC-IM-MS) and performing isotopic envelopes detection.  DIA-Umpire - Data Independent Acquisition (DIA) data processing tool that doesn't require a spectral library (Java).  ","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"1fc8272d8eb68535abe1b8276575e87e","permalink":"https://dmtavt.com/authors/chhh/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/chhh/","section":"authors","summary":"Best way to describe myself would be: a software engineer with a lot of hands on experience in mass-spectrometry (liquid and gas chromatography, ion mobility).\nUsual course of action - acquire new data, explore it with visualizations, come up with a suitable processing algorithm. In most cases this results in desktop GUI applications for the viz part and CLI programs to perform batch data processing.\n Prefer writing code in Java or C#, with occasional forays into C++.","tags":null,"title":"Dmitry Avtonomov","type":"authors"},{"authors":[],"categories":[],"content":"Windows environment It's best to configure OpenSSH Authentication Agent service to automatically start. Alternatively, you can start it manually every time when opening powershell for the first time:\nStart-Service ssh-agent  To have SSH agent to automatically start with Windows, you can run (from elevated powershell prompt):\nSet-Service ssh-agent -StartupType Automatic  After that, you need to add your ssh key once:\nssh-add C:\\Users\\your-name\\ssh\\id_rsa  Now everytime the ssh-agent is started, the key will be there. You can check which keys are registered with the ssh-agent:\nssh-add -l  Credit: https://superuser.com/questions/1327633/how-to-maintain-ssh-agent-login-session-with-windows-10s-new-openssh-and-powers\nLinux-like environement On linux or in git-for-windows environment, I use the following snippet in my .bashrc to achieve the same effect:\n# This is used to start ssh-agent once when git-bash is started. # Saves typing the ssh key password every time you interact with # a remote repo. env=~/.ssh/agent.env agent_load_env () { test -f \u0026quot;$env\u0026quot; \u0026amp;\u0026amp; . \u0026quot;$env\u0026quot; \u0026gt;| /dev/null ; } agent_start () { (umask 077; ssh-agent \u0026gt;| \u0026quot;$env\u0026quot;) . \u0026quot;$env\u0026quot; \u0026gt;| /dev/null ; } agent_load_env # agent_run_state: 0=agent running w/ key; 1=agent w/o key; 2= agent not running agent_run_state=$(ssh-add -l \u0026gt;| /dev/null 2\u0026gt;\u0026amp;1; echo $?) if [ ! \u0026quot;$SSH_AUTH_SOCK\u0026quot; ] || [ $agent_run_state = 2 ]; then agent_start ssh-add elif [ \u0026quot;$SSH_AUTH_SOCK\u0026quot; ] \u0026amp;\u0026amp; [ $agent_run_state = 1 ]; then ssh-add fi unset env  ","date":1596480578,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596480578,"objectID":"b4aed630bcfed1535c44bfaedad2ca7b","permalink":"https://dmtavt.com/post/2020-08-03-ssh-agent-powershell/","publishdate":"2020-08-03T11:49:38-07:00","relpermalink":"/post/2020-08-03-ssh-agent-powershell/","section":"post","summary":"Start ssh-agent once when powershell or git-bash is started. Saves typing the ssh key password every time you interact with a remote repo.","tags":["powershell","ssh","ssh-agent"],"title":"Automatically starting ssh-agent when powershell or git-bash are started","type":"post"},{"authors":[],"categories":["java","csharp"],"content":"Preamble Many programs do some sort of data transform and can be described as read/generate some data, process the data, write the output. It's always beneficial if some steps are performed in parallel. E.g. the reader pre-fetches a few data items so that when the \u0026lsquo;processing\u0026rsquo; part of the program is ready for a new chunk of data the data is already there waiting. Ths post provides two quick solutions for java and C#. Java with Project Reactor . C# using TPL Dataflow (Task Parallel Library).\nC# TPL Dataflow The example code produces (reads etc.) new items concurrently with processing said items, maintaining a read-ahead buffer. The completion signal is sent to the head of the chain when the \u0026ldquo;producer\u0026rdquo; has no more items. The program also awaits the completion of the whole chain before terminating.\nPosted in this StackOverflow thread on the topic.\nstatic async Task Main() { string Time() =\u0026gt; $\u0026quot;{DateTime.Now:hh:mm:ss.fff}\u0026quot;; // the buffer is added to the chain just for demonstration purposes // the chain would work fine using just the built-in input buffer // of the `action` block. var buffer = new BufferBlock\u0026lt;int\u0026gt;(new DataflowBlockOptions {BoundedCapacity = 3}); var action = new ActionBlock\u0026lt;int\u0026gt;(async i =\u0026gt; { Console.WriteLine($\u0026quot;[{Time()}]: Processing: {i}\u0026quot;); await Task.Delay(500); }, new ExecutionDataflowBlockOptions {MaxDegreeOfParallelism = 2, BoundedCapacity = 2}); // it's necessary to set `PropagateCompletion` property buffer.LinkTo(action, new DataflowLinkOptions {PropagateCompletion = true}); //Producer foreach (var i in Enumerable.Range(0, 10)) { Console.WriteLine($\u0026quot;[{Time()}]: Ready to send: {i}\u0026quot;); await buffer.SendAsync(i); Console.WriteLine($\u0026quot;[{Time()}]: Sent: {i}\u0026quot;); } // we call `.Complete()` on the head of the chain and it's propagated forward buffer.Complete(); await action.Completion; }  Java, Project Reactor Reactive processing: async IO producer with prefetch and in-order consumers (Project Reactor 3.x). Posted to this StackOverflow thread .\nProblem statement: Do I/O in chunks. Start processing chunks as soon as one becomes available, while further chunks are being read in background (but not more than X chunks are read ahead). Process chunks in parallel as they are being received. Consume each processed chunk in-order-of-reading, i.e. in original order of the chunk being read.\nPseudo-Rx code explanation of what we'd like to achieve: Flux.fromFile(path, some-function-to-define-chunk) // done with Flux.generate in MWE below .prefetchOnIoThread(x-count: int) // at this point we try to maintain a buffer filled with x-count pre-read chunks .parallelMapOrdered(n-threads: int, limit-process-ahead: int) // n-threads: are constantly trying to drain the x-count buffer, doing some transformation // limit-process-ahead: as the operation results are needed in order, if we encounter an // input element that takes a while to process, we don't want the pipeline to run too far // ahead of this problematic element (to not overflow the buffers and use too much memory) .consume(TMapped v)  My solution final int threads = 2; final int prefetch = 3; Flux\u0026lt;Integer\u0026gt; gen = Flux.generate(AtomicInteger::new, (ai, sink) -\u0026gt; { int i = ai.incrementAndGet(); if (i \u0026gt; 10) { sink.complete(); } else { sink.next(i); } return ai; }); gen.parallel(threads, prefetch) // switch to parallel processing after genrator .runOn(Schedulers.parallel(), prefetch) // if you don't do this, it won't run in parallel .map(i -\u0026gt; i + 1000) // this is done in parallel .ordered(Integer::compareTo) // you can do just .sequential(), which is faster .subscribeOn(Schedulers.elastic()) // generator will run on this scheduler (once subscribed) .subscribe(i -\u0026gt; { System.out.printf(\u0026quot;Transformed integer: \u0026quot; + i); // do something with generated and processed item });  ","date":1592414257,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592414257,"objectID":"9550340f7d4cc433971386a8a12165b6","permalink":"https://dmtavt.com/post/2020-06-17_parallel-producer-consumer-queues/","publishdate":"2020-06-17T10:17:37-07:00","relpermalink":"/post/2020-06-17_parallel-producer-consumer-queues/","section":"post","summary":"Many programs do some sort of data transform and can be described as read/generate some data, process the data, write the output. It's always beneficial if some steps are performed in parallel. E.g. the reader pre-fetches a few data items so that when the 'processing' part of the program is ready for a new chunk of data the data is already there waiting. Ths post provides two quick solutions for java and C#. Java with Project Reactor. C# using TPL Dataflow (Task Parallel Library).","tags":["java","csharp","queue","pipeline","reactive"],"title":"Parallel Producer Consumer Queues","type":"post"},{"authors":[],"categories":[],"content":"Symptopms  After first reboot of VM destop of guest OS stopped auto-resizing to fill host window. But it was working fine during guest OS install, or while it was running from live cd.  TL;DR; Restart vmtoolsd service.\nOn Manjaro / Arch:\nsudo systemctl restart vmtoolsd\nHas to be done after every boot-up. And sometimes after session locks out as well.\nSome suggest that delaying the start of the service helps:\nsudo vim /etc/systemd/system/multi-user.target.wants/vmtoolsd.service\nand add the folowing in [Unit] section:\n[Unit] After=graphical.target  Didn't help me.\nA bit more context VMWare Workstation requires you to have open-vm-tools package service installed, which actually does the resizing. I was running Manjaro 20 under VMWare Workstation 15 on Win10 Pro. Open VM Tools was already pre-installed with Manjaro distribution.\nHowever it seems like the service named vmtoolsd starts too early. On the internet everyone mentions open-vm-tools, but it probably changed name to vmtoolsd.\nPossible solutions https://github.com/vmware/open-vm-tools/issues/253 https://github.com/vmware/open-vm-tools/issues/303\n","date":1589441007,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589441007,"objectID":"724f67c585e06f555a565ede8b7f28f8","permalink":"https://dmtavt.com/post/2020-05-14-auto-resize-guest-vm-desktop/","publishdate":"2020-05-14T00:23:27-07:00","relpermalink":"/post/2020-05-14-auto-resize-guest-vm-desktop/","section":"post","summary":"In VMWare Workstation guest OS desktop doesn't auto-resize to fill host OS window after first reboot. Solution.","tags":["vmware","manjaro","open-vm-tools","vmtoolsd"],"title":"Auto Resize Guest VM Desktop running under VMWare","type":"post"},{"authors":[],"categories":[],"content":"Solution origin - awesome Manjaro forums https://forum.manjaro.org/t/installing-expressvpn-on-manjaro-how-to/125345/9\nSolution Remove old install (optional) Stop and remove old install of expressvpn, if present:\nexpressvpn disconnect sudo systemctl stop expressvpn sudo systemctl disable expressvpn sudo pacman -Rns expressvpn  Install new package from ExpressVPN website ExpressVPN website provides a package for Arch now. Manjaro is based on Arch, so that's what we need.\nDownload 64 bit .pkg.tar.xz Arch package: https://www.expressvpn.com/setup#linux\nif you have a fresh install of manjaro:\n  go to expressvpn and download the Arch 64 bit version and copy the activation code\n  install it via pacman\n sudo pacman -U /path/to/expressvpn.package.tar.xz    copy the service scripts to the correct location\n sudo cp /usr/lib/expressvpn/expressvpn*.service /etc/systemd/system/.    enable and start the service\n sudo systemctl enable expressvpn sudo systemctl start expressvpn    activate it with the activation code you copied earlier\n expressvpn activate    connect vpn\n expressvpn connect smart    When you connect, expressvpn gives you some valuable infor you might miss:\n To check your connection status, type expressvpn status. If your VPN connection unexpectedly drops, internet traffic will be blocked to protect your privacy. To disable Network Lock, disconnect ExpressVPN then type \u0026lsquo;expressvpn preferences set network_lock off\u0026rsquo;.  ","date":1589315418,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589315418,"objectID":"10845a347e657ae9e391bbb793d7b13b","permalink":"https://dmtavt.com/post/2020-05-13-expressvpn-on-manjaro/","publishdate":"2020-05-12T12:30:18-08:00","relpermalink":"/post/2020-05-13-expressvpn-on-manjaro/","section":"post","summary":"How to install expressvpn client on a fresh install of Manjaro / Arch","tags":["howto","expressvpn","manjaro","arch-linux"],"title":"Installing ExpressVPN client on Manjaro Linux","type":"post"},{"authors":[],"categories":[],"content":"Symptoms  VMWare Wrokstation Player shows a warning when you try to create a new 64-bit VM on a 64-bit host (Win10 Pro host in my case) VMWare Wrokstation Player just starts the vm, shows black screen and then an error message pops up, saying: VMWare Workstation and Device/Credential Guard are not compatible.  TL;DR;  Disable Hyper-V and Windows Sandbox in Windows Features (Control Panel -\u0026gt; Programs -\u0026gt; Turn Windows Features On or Off) Enable processor virtualization in BIOS From elevated CMD (run as administrator): bcdedit /set hypervisorlaunchtype off\rshutdown /r /t 0\r   Some details of what might be wrong  Make sure vitualization is turned on for the processor in BIOS (this step varies by BIOS manufacturer) In Windows\u0026rsquo; features check that Hyper-V and Windows Sandbox are not enabled.  My experience I had Windows Sandbox enabled in the first place. This was causing VMWare Workstation 15.5.2 to show the warning about 64-bit guest on 32-bit host. Even though the host was 64-bit in fact. I then uninstalled Windows Sandbox via Add/Remove Windows Features menu. Then I started getting the more common Device/Credential Guard are not compatible error message.\nRunning from elevated CMD:\n bcdedit /set hypervisorlaunchtype off\rshutdown /r /t 0\r Did the final trick.\n","date":1589313618,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589313618,"objectID":"821bf26ef64ab80e80e8511bdec038ef","permalink":"https://dmtavt.com/post/2020-05-13-vmware-player-fix/","publishdate":"2020-05-12T12:00:18-08:00","relpermalink":"/post/2020-05-13-vmware-player-fix/","section":"post","summary":" ","tags":["fixme","howto","vmware","windows"],"title":"Fixing VMWare Workstation `Device/Credential Guard are not compatible` and `64-bit Guest OS can't run on 32-bit host` errors","type":"post"},{"authors":[],"categories":[],"content":"This will skip all the unnecessary stuff, like .git directory, and files ignored in the git repo itself:\ngit archive --format=zip -o archive-name.zip HEAD\nYou can omit --format.zip when you specify output files via -o, the format will be inferred.\nAdd --prefix subdir-name to have all files in the archive be put into a folder inside the archive, e.g:\ngit archive --prefix subdir-name/ -o archive-name.zip HEAD\n","date":1580092458,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580092458,"objectID":"4218052e1b32724d852b1906e97b3730","permalink":"https://dmtavt.com/post/2020-01-26-zipping-git-repo/","publishdate":"2020-01-26T18:34:18-08:00","relpermalink":"/post/2020-01-26-zipping-git-repo/","section":"post","summary":"How to make a zip copy of all git repo files without garbage","tags":["git","howto"],"title":"Zipping all relevant files in a git repo","type":"post"},{"authors":["David J Clark","Saravana M Dhanasekaran","Francesca Petralia","Jianbo Pan","Xiaoyu Song","Yingwei Hu","Felipe da Veiga Leprevost","Boris Reva","Tung-Shing M Lih","Hui-Yin Chang"," others"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"5775aff5dc953eb8d16fc7f7573547bc","permalink":"https://dmtavt.com/publication/clark-2019-integrated/","publishdate":"2020-01-26T23:59:57.843878Z","relpermalink":"/publication/clark-2019-integrated/","section":"publication","summary":"","tags":null,"title":"Integrated proteogenomic characterization of clear cell renal cell carcinoma","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"77902723a332ac93065f28bcc904739d","permalink":"https://dmtavt.com/moar/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/moar/","section":"","summary":"Hello!","tags":null,"title":"Landing Page","type":"widget_page"},{"authors":null,"categories":null,"content":"You have a git repo (a directory) which contains sub-directories which are git repos themselves and now you want them to become submodules. Here's a quick automatic way to add them all.\n Remove -maxdepth if you want to recursively dig deeper than 1 level. Assumes that each submodule has a remote named origin, uses that remote's url.  Preview command for x in $(find . -mindepth 1 -maxdepth 1 -type d) ; do if [ -d \u0026quot;${x}/.git\u0026quot; ] ; then cd \u0026quot;${x}\u0026quot; ; origin=\u0026quot;$(git config --get remote.origin.url)\u0026quot; ; cd - 1\u0026gt;/dev/null; echo git submodule add \u0026quot;${origin}\u0026quot; \u0026quot;${x}\u0026quot; ; fi ; done\r Will only print the submodule commands to be executed, but NOT execute them.\nActual command for x in $(find . -mindepth 1 -maxdepth 1 -type d) ; do if [ -d \u0026quot;${x}/.git\u0026quot; ] ; then cd \u0026quot;${x}\u0026quot; ; origin=\u0026quot;$(git config --get remote.origin.url)\u0026quot; ; cd - 1\u0026gt;/dev/null; git submodule add \u0026quot;${origin}\u0026quot; \u0026quot;${x}\u0026quot; ; fi ; done\r Will execute the actual submodule commands.\nReadable versions  Preview  for x in $(find . -mindepth 1 -maxdepth 1 -type d) ; do if [ -d \u0026quot;${x}/.git\u0026quot; ] ;\rthen cd \u0026quot;${x}\u0026quot; ;\rorigin=\u0026quot;$(git config --get remote.origin.url)\u0026quot; ;\rcd - 1\u0026gt;/dev/null;\recho git submodule add \u0026quot;${origin}\u0026quot; \u0026quot;${x}\u0026quot; ;\rfi ;\rdone\r  Execute  for x in $(find . -mindepth 1 -maxdepth 1 -type d) ; do if [ -d \u0026quot;${x}/.git\u0026quot; ] ; then cd \u0026quot;${x}\u0026quot; ; origin=\u0026quot;$(git config --get remote.origin.url)\u0026quot; ; cd - 1\u0026gt;/dev/null; git submodule add \u0026quot;${origin}\u0026quot; \u0026quot;${x}\u0026quot; ; fi ; done\r Credit (modified from an answer here):\nhttps://stackoverflow.com/questions/10606101/automatically-add-all-submodules-to-a-repo\n","date":1543132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543132800,"objectID":"4401b24c976b1575a0c20f82e7a23be7","permalink":"https://dmtavt.com/post/2018-11-25-adding-all-git-submodules/","publishdate":"2018-11-25T00:00:00-08:00","relpermalink":"/post/2018-11-25-adding-all-git-submodules/","section":"post","summary":"\r\nYou have a git repo which contains sub-directories which are git repos\r\nthemselves and now you want them to become submodules. Here's a quick automatic\r\nway to add them all.\r\n","tags":["git","submodule","git-submodule","howto"],"title":"Adding all existing submodules to a git repo","type":"post"},{"authors":null,"categories":null,"content":"We'll be using scoop to install the necessary tools.\nQuickstart with scoop  Install git separately. Here's the link: https://gitforwindows.org. I recommend the following customizations in the installer:  Use Notepad++ as default editor Use True Type fonts in all console windows Check out as-is, commit as-is   Win+ R -\u0026gt; powershell. In PowerShell (answer \u0026lsquo;Y\u0026rsquo; when asked):  Set-ExecutionPolicy RemoteSigned -scope CurrentUser iex (new-object net.webclient).downloadstring('https://get.scoop.sh')   scoop is now installed, if you had any open cmd windows - close them now.  Colors for cmd.exe and PowerShell Install concfg using scoop or from here: https://github.com/lukesampson/concfg\nscoop install concfg  Command to import the colorscheme:\nconcfg import \u0026lt;path-to-json\u0026gt;  Color scheme as a gihib gist:\n ","date":1539241200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539241200,"objectID":"81c275a88270299f8f2eba1c34e9cfb9","permalink":"https://dmtavt.com/post/2018-10-11-custom-colors-in-win-shells/","publishdate":"2018-10-11T00:00:00-07:00","relpermalink":"/post/2018-10-11-custom-colors-in-win-shells/","section":"post","summary":"\r\nWouldn't it be nice if you had completion and nice colors in cmd.exe\r\njust like what you get out of the box on most Linuxes?\r\n","tags":["shell","colors"],"title":"Custom nice colors in Windows shells","type":"post"},{"authors":null,"categories":null,"content":"Setting up Raspberry Pi Download the OMV image (yes, the binaries are hosted at sourceforge still) and burn it to an SD card using Etcher .\nDefault loing/password is (IIRC): admin/openmediavault. I highly recommend setting up a preferred fixed IP address for you Pi on the router.\nUsing all the space on the system SD card I used a 64GB card, but OMV only needs a gig or so. Reboot once, the partition for OMV will be be auto-resized. A third partition on the SD card will be created, but without a filesystem, check it out with lsblk command. Remember the name of the partition.\nTo add a filesystem to this partition use :\nmkfs.ext4 -L \u0026lt;your-label\u0026gt; /dev/\u0026lt;the-partition-name\u0026gt;\r You can now go to the Web Console -\u0026gt; Storage -\u0026gt; File Systems, locate the new partition and Mount it. It can now be used to store the data.\nSetting up an HDD I have a 1TB 2.5\u0026rdquo; HDD (HGST Travelstar 7K1000, Amazon link ) in a simple USB 3 enclosure. The Pi struggled to power the HDD though, so I bought an powered USB hub (Anker 4-port powered USB hub, Amazon link ). Haven't tried it yet, but it's quite possible you can power up to 4 HDDs this way and use them all with a single Pi.\nJust plug the HDD in the hub, the hub into wall power and the Pi - you're ready to go. Reboot the Pi. Go to the web admin console. Storage -\u0026gt; Disks, check that you see the new HDD there, probably as /dev/sda1.Storage -\u0026gt; File Systems\r-\u0026gt; +Create (there's a button in the top row), create a new ext4 filesystem on the HDD.\nCreating a shared folder available from your local network To enable file sharing using this HDD check that Services -\u0026gt; SMB/CIFS is enabled. Access Right Management -\u0026gt; Shared Folders -\u0026gt; +Add and add a folder on the hard drive. You might also need to go to Access Right Management -\u0026gt; User and add a new user, click Privileges and grant the new user access to the shared folder.\nOn Windows you can now open Explorer and Map network drive. Put in an address like \\\\192.168.1.176\\my-share. If all was right, it should ask you for login/password. Check the box log in using different credentials and supply the username and password for the user that you've just created through OMV admin web page.\nSetting up backup of your cloud-based storage to the local HDD connected to Pi Log in to RPi from ssh. Install RClone (https://rclone.org). Worked fine from the script installation for me. Run rclone configure to set up Google Drive integration. When asked to do \u0026ldquo;interactive\u0026rdquo; setup, say No. Then you can just copy the given link and follow it on another computer with an actual browser. Lynx won't do, as the Google authentication page needs javascript.\nGive it a go while ssh'ed into the Pi. When everything works, you can go to the OMV admin panel System -\u0026gt; Scheduled Jobs, which is basically a UI for adding cron jobs, and add your rclone sync or rclone copy command to run, e.g. every night at 3AM.\n","date":1538377200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538377200,"objectID":"6bbe626cfd83f2e62ff4f57d2f51f755","permalink":"https://dmtavt.com/post/2018-10-01-install-omv-on-pi/","publishdate":"2018-10-01T00:00:00-07:00","relpermalink":"/post/2018-10-01-install-omv-on-pi/","section":"post","summary":"\r\nWalkthrough reminder for myself how to install OMV on Pi and set \r\nup backup to an external HDD.\r\n","tags":["omv","openmediavault","pi","raspberry-pi"],"title":"Installing OMV 4 (Open Media Vault) to Raspberry Pi 3+","type":"post"},{"authors":null,"categories":null,"content":"","date":1536364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536364800,"objectID":"11bbfcea5933601fcfd462f3035cc013","permalink":"https://dmtavt.com/graphic/so-hood-day/","publishdate":"2018-09-08T00:00:00Z","relpermalink":"/graphic/so-hood-day/","section":"graphic","summary":"","tags":["shirt"],"title":"So Hood (Day)","type":"graphic"},{"authors":null,"categories":null,"content":"","date":1536364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536364800,"objectID":"5bd2cdab1cc0342930a4064b932fe960","permalink":"https://dmtavt.com/graphic/so-hood-night/","publishdate":"2018-09-08T00:00:00Z","relpermalink":"/graphic/so-hood-night/","section":"graphic","summary":"","tags":["shirt"],"title":"So Hood (Night)","type":"graphic"},{"authors":null,"categories":null,"content":"","date":1536364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536364800,"objectID":"3932fcd19a8b55409eecfbc9fc286941","permalink":"https://dmtavt.com/graphic/cat-teleportation-mug/","publishdate":"2018-09-08T00:00:00Z","relpermalink":"/graphic/cat-teleportation-mug/","section":"graphic","summary":"","tags":["mug"],"title":"The theory of cat teleportation","type":"graphic"},{"authors":null,"categories":null,"content":"","date":1536364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536364800,"objectID":"d4bbd394c0c5048dafeef675a4083132","permalink":"https://dmtavt.com/graphic/whats-above-california/","publishdate":"2018-09-08T00:00:00Z","relpermalink":"/graphic/whats-above-california/","section":"graphic","summary":"","tags":["shirt"],"title":"What's Above California?","type":"graphic"},{"authors":null,"categories":null,"content":"A while ago this took more effort as described in this link (scroll to section \u0026lsquo;External Merge and Diff Tools\u0026rsquo;).\nNot anymore. Now git natively supports p4merge. So you only need to modify your global git config (~/.gitconfig):\n[merge]\rtool = p4merge\r[mergetool \u0026quot;p4merge\u0026quot;]\rpath = p4merge.exe\r[diff]\rtool = p4merge\r[difftool \u0026quot;p4merge\u0026quot;]\rpath = p4merge.exe\r You can provide the absolute path to the p4merge binary, but I have it on PATH because I installed it with scoop .\nInstall p4merge with scoop To get scoop, from the PowerShell execute:\nSet-ExecutionPolicy RemoteSigned -scope CurrentUser\riex (new-object net.webclient).downloadstring('https://get.scoop.sh')\r p4merge is in the extras scoop bucket , add the bucket first to be able to install it easily:\nscoop bucket add extras\r then you can install p4merge\nscoop update\rscoop search p4merge\rscoop install p4merge\r To list all you installed scoop apps with PowerShell:\n(scoop list) | sls '^ (\\w+)' |% { $_.matches.groups[1].value }\r Full .gitconfig My full .gitconfig is below. It [include]s two paths .gitaliases and .gitmorealiases, they are both just text files in my home directory, i.e. they are siblings of my global .gitconfig file. They contain, you guessed it, git aliases. A large collection of aliases is available here . And here is a very useful blog post / tutorial introducing you to the usefulness of aliases.\n\r","date":1536303600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536303600,"objectID":"f2867769feea6dd029c34db64963f6d8","permalink":"https://dmtavt.com/post/2018-09-07-p4merge-as-git-merge-tool/","publishdate":"2018-09-07T00:00:00-07:00","relpermalink":"/post/2018-09-07-p4merge-as-git-merge-tool/","section":"post","summary":"\r\nResolving merge conflicts is one the things that I hate most. \r\nLets make it a little easier with p4merge.\r\n","tags":["git","gitconfig","mergetool","difftool"],"title":"Use p4merge from Perforce as default git mergetool","type":"post"},{"authors":null,"categories":null,"content":"","date":1536278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536278400,"objectID":"3beed7902efaacf4d90d88144fdd5283","permalink":"https://dmtavt.com/graphic/cat-teleportation-shirt/","publishdate":"2018-09-07T00:00:00Z","relpermalink":"/graphic/cat-teleportation-shirt/","section":"graphic","summary":"","tags":["shirt"],"title":"The theory of cat teleportation","type":"graphic"},{"authors":null,"categories":null,"content":"It's a simple registry hack.\n Strat registry editor: Run -\u0026gt; regedit.   Go to HKEY_CLASSES_ROOT\\Directory\\shell\\cmd. Right-click the cmd (folder) key, and click Permissions. Click the Advanced button. Advanced Security Settings dialog will open, at the top next to Owner click Change. Put in your account name, click Check Names to make sure domain etc is set up correctly, click OK. Check the Replace owner on subcontainers and objects check box near Owner field. Click Apply, OK. In the now open Premissions dialog, select Administrators, check Full Control checkbox below. Click Apply, OK. You are now back in the registry editor. Click the cmd folder on the left. Look for HideBasedOnVelocityId key in the right window tab.  If it's there, Right-click, Rename to ShowBasedOnVelocityId. If it's not there, just add the ShowBasedOnVelocityId key (type DWORD), value 0x00639bc8 (6527944).   Now try Shift + Right Click in an Explorer window, you should see the Open command window here option.  ","date":1530082800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530082800,"objectID":"1695dee9aa0850479886001968fb39fe","permalink":"https://dmtavt.com/post/2018-06-27-command-prompt-here/","publishdate":"2018-06-27T00:00:00-07:00","relpermalink":"/post/2018-06-27-command-prompt-here/","section":"post","summary":"`Shift + Right Click` in an explorer window in Win10 now has the option to open\nPowerShell. If you want to have cmd.exe back instead, here's how, using the the\nregistry.\n","tags":["windows-tooling"],"title":"Bring back \"Open command window here\" context menu item in Windows","type":"post"},{"authors":null,"categories":null,"content":"Intro The whole process for various situations is described in GitHub documentation . This is just a reminder for myself how to properly configure top-level apex domain redirect with GitHub Pages.\nAll of your \u0026lt;username\u0026gt;.github.io or \u0026lt;orgname\u0026gt;.github.io pages will be redirected to the new custom domain, so your project pages can benefit from this automatically. If you already have a project page like \u0026lt;username\u0026gt;.github.io/projectname, this will become \u0026lt;custom.domain\u0026gt;/projectname.\nSequence of actions Let's say you own domain.com for the sake of example, and your github username is yourname.\nOn GitHub Create a repository with name \u0026lt;username\u0026gt;.github.io or \u0026lt;orgname\u0026gt;.github.io depending on the repo type.\nCreate a repo yourname.github.io add a file called CNAME with the following contents to the root directory:\ndomain.com\rwww.domain.com\r If you prefer the name with www prefix, put it at the top. Commit and push. That's it with github.\nAt DNS provider config Add an A record:\nname: @\rtype: A\rTTL : 1h\rdata: [185.199.108.153, 185.199.109.153, 185.199.110.153, 185.199.111.153]\r Add a CNAME record:\nname: www\rtype: CNAME\rTTL : 1h\rdata: yourname.github.io\r With domains.google.com this looks like the following:\nIt might take several hours for the information to propagate throughout the net and become accessible everywhere.\n","date":1525676400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525676400,"objectID":"f90779aefb3f7a8dbd37bb36ff6e0c4c","permalink":"https://dmtavt.com/post/2018-05-07-custom-domain-name-with-github-pages/","publishdate":"2018-05-07T00:00:00-07:00","relpermalink":"/post/2018-05-07-custom-domain-name-with-github-pages/","section":"post","summary":"\r\nIf you host a user/org website on github-pages you might want to change the\r\ndefault `username.github.io` to something nicer, like `username.com`.\r\nThis is a short instruction how to set up the repo and DNS configuration\r\nwith your DNS provider to achieve this.\r\n","tags":["github"],"title":"Set up custom domain name with GitHub pages","type":"post"},{"authors":null,"categories":null,"content":"Shells to be used:\n cmd.exe MINGW from Git   Cygwin  Optionally MSYS2   All of the above are managed through one shell manager - ConEmu .\nFirst install all the shells, then ConEmu.\nEasy installation of useful dev tools Use scoop to install basic utilities and languages. Install from powershell (without admin rights):\nSet-ExecutionPolicy RemoteSigned -scope CurrentUser iex (new-object net.webclient).downloadstring('https://get.scoop.sh')  Using command scoop export you can get an importable list of all installed packages. Here's mine currently: scoop-export.txt .\nTweaks to shells cmd.exe To get auto-completion support similar to what you get in bash, install clink .\nConEmu Win + Alt + P to open Properties. Startup -\u0026gt; Tasks and set shortcuts for shells you'll be using.\n Alt + C for cmd.exe Alt + Shift + C for elevated cmd.exe Alt + W for MINGW from git Alt + Y for Cygwin  The full config is here , can be imported at installation time.\nCygwin Basic packages to install throught setup.exe:\n wget curl lynx zip unzip  There is little known package manager that can be used from inside cygwin without having to run setup.exe all the time: apt-cyg .\nInstallation:\nlynx -source rawgit.com/transcode-open/apt-cyg/master/apt-cyg \u0026gt; apt-cyg install apt-cyg /bin  ","date":1520409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525330800,"objectID":"dda86db9c7fcbf45a643a74e37ccaadb","permalink":"https://dmtavt.com/post/2018-03-07-shells-on-windows/","publishdate":"2018-03-07T00:00:00-08:00","relpermalink":"/post/2018-03-07-shells-on-windows/","section":"post","summary":"\r\nConfiguring various shells and convenient dev environments on Windows with ConEmu.\r\n","tags":["shell","windows-tooling"],"title":"Using multiple different shells/subsystems on Windows","type":"post"},{"authors":null,"categories":null,"content":"Once I tried to connect from Win10 to a hotspot running off Google Pixel 2 (Android 8.1.0) connected to T-Mobile (tethering) and it worked for a while, but one day it just broke. Connection would work for 5-10 seconds allowing to load one website maybe, but then it threw some DNS errors. Setting DNS server manually didn't help, also ping 8.8.8.8 timed out.\nThe solution (from elevated cmd prompt):\nnetsh int ip reset c:\\resetlog.txt netsh winsock reset catalog ipconfig /flushdns  And restart the computer.\nTaken from this question @ answers.microsoft.com .\n","date":1519027200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519027200,"objectID":"bcf4b873c1093d51202d53330a0ad50e","permalink":"https://dmtavt.com/post/2018-02-19-tethering-wifi-from-android-dns-problems/","publishdate":"2018-02-19T00:00:00-08:00","relpermalink":"/post/2018-02-19-tethering-wifi-from-android-dns-problems/","section":"post","summary":"\r\nIf your tethered (from a phone to a *windows* computer) internet connection\r\ndrops after a few seconds of being established, this post might be for you.\r\n","tags":["android","wifi","tethering"],"title":"Solving problems with tethering wifi from an Android phone","type":"post"},{"authors":["Dmitry M Avtonomov","Andy Kong","Alexey I Nesvizhskii"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"8222786dc547cae38de3633505913c26","permalink":"https://dmtavt.com/publication/avtonomov-2018-deltamass/","publishdate":"2020-01-26T23:59:57.841876Z","relpermalink":"/publication/avtonomov-2018-deltamass/","section":"publication","summary":"","tags":null,"title":"DeltaMass: Automated Detection and Visualization of Mass Shifts in Proteomic Open-Search Results","type":"publication"},{"authors":["Dmitry M Avtonomov","Daniel A Polasky","Brandon T Ruotolo","Alexey I Nesvizhskii"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"5a3b35de74033ad018a5a0eb69ef5045","permalink":"https://dmtavt.com/publication/avtonomov-2018-imtbx/","publishdate":"2020-01-26T23:59:57.840876Z","relpermalink":"/publication/avtonomov-2018-imtbx/","section":"publication","summary":"","tags":null,"title":"IMTBX and Grppr: Software for Top-Down Proteomics Utilizing Ion Mobility-Mass Spectrometry","type":"publication"},{"authors":null,"categories":[],"content":" Intellij IDEA IDE has a nice feature of live templates. You type several symbols, press tab (by default) and it expands the symbols into full expressions. E.g. you can type fori, press Tab and get a full for loop created for you, or type iter, press Tab and if you have iterables in scope, you'll be presented with a choice of available ones to iterate over.\nExample: slf4j logger in each class It's a super common thing to type at the beginning of each class:\nprivate static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MyClassName.class);  pretty tedious even with IDE's help.\nLive templates to the rescue, you can define your own one to create the whole line for you! You\nHow to define a live template  Go to File-\u0026gt;_Settings_-\u0026gt;_Editor_-\u0026gt;_Live Templates_. In the right panel tree select category other. Click the plus (+) sign on the top right, select Live Template. Set  Abbreviation: log Description: Inserts private static Logger for slf4j Template text: private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger($CLASS_NAME$.class);$END$   Now click the Edit Variables button, we will tell the IDE what $CLASS_NAME$ means here. $END$ means where to place the cursor after template expansion.  Name: CLASS_NAME Expression: className() Default value: leave empty Skip if defined: true (check the checkbox)   At the very bottom look for text Applicable in with a link Change next to it, click it. Select Java-\u0026gt;_declaration_.  Congratulations, you're done! Just type log and press Tab anywhere in the class declaration.\n","date":1485388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485388800,"objectID":"5d5c46ddcff65130cf4f8f705f80574a","permalink":"https://dmtavt.com/post/2017-01-26-intellij-idea-live-templates-slf4j/","publishdate":"2017-01-26T00:00:00Z","relpermalink":"/post/2017-01-26-intellij-idea-live-templates-slf4j/","section":"post","summary":" Configure your own templates for daunting repetitive tasks in IDEA, like defining a logger variable each time. After reading this post you'll be able to type `log` press TAB and get the full logger declaration for _slf4j_.","tags":["intellij"],"title":"Intellij IDEA live templates","type":"post"},{"authors":null,"categories":[],"content":"Table of Contents  What is Sonatype and why is it needed? Getting started GPG signing  Windows caveat   Configuring Maven to know where to get the signing key Configuring Maven to know the credentials for Sonatype servers Satisfying requirements to pass all checks upon submission to Sonatype Publishing parent POM project to Central Publishing the project to Central   What is Sonatype and why is it needed? Sonatypeis a company that provides a staging repository, which performs validation and allows to push the builds that pass all checks to the Central repo. Without it, basically, you can't easily publish anything to the Central easily, unless you're an Apache project or similar.\nGetting started Follow their getting started guide to set up the needed credentials. This should be easy - you create a JIRA account and you create a ticket in JIRA to claim your namespace (groupId in Maven terms). If you have a github account, for example,  http://github.com/chhh , you'll want to claim com.github.chhh.\nGPG signing You'll need to set up and publish your GPG key for signing artifacts. This is described here .\nIn short you'll need to install gpg or gpg2. I did it on Windows and already had a working gpg that came with git installation. So I happily used that to generate my key with (create it with a passphrase!):\ngpg --gen-key\nMake sure to check that the generated key does not have sub keys for signing. First issue gpg --list-keys, the output should be like:\n$ gpg --list-keys /c/Users/\u0026lt;username\u0026gt;/.gnupg/pubring.gpg --------------------------------- pub 2048R/DA123C12 2012-01-24 uid Dmitry Avtonomov (chhh) \u0026lt;email@gmail.com\u0026gt; sub 2048R/3E123123 2012-01-24  Notice, that there is one pub key and one sub key. You want this sub key to not be Usage: C type. gpg --edit-key DA123C12\ngpg (GnuPG) 1.4.20; Copyright (C) 2015 Free Software Foundation, Inc. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Secret key is available. pub 2048R/DA100C23 created: 2012-01-24 expires: never usage: SC trust: ultimate validity: ultimate sub 2048R/3E123123 created: 2012-01-24 expires: never usage: E [ultimate] (1). Dmitry Avtonomov (chhh) \u0026lt;email@gmail.com\u0026gt;  In this case the sub key is usage: E, it's used for encryption only, so we're good to go, otherwise you'd need to delete or revoke it. Published the key with:\ngpg --keyserver hkp://pool.sks-keyservers.net --send-keys \u0026lt;key-id\u0026gt;\nWindows caveat The previous steps created the keychain file in c:/Users/\u0026lt;username\u0026gt;/.gnupg. However, when I later installed the native windows gpg from https://www.gnupg.org/download/ I've found that it used a different default path and I could not list the key anymore. Addind a new environment variable GNUPGHOME and set it to C:\\Users\\\u0026lt;username\u0026gt;\\.gnupg. Now the gpg that was installed in windows could read the old keychain, which meant maven could now use that key to sign files.\nConfiguring Maven to know where to get the signing key Check out your \u0026lt;maven-install-apth\u0026gt;/conf directory. There should be a settings.xml file. Copy that over to your \u0026lt;user-home\u0026gt;/.m2, unless you already have it there. Add the following to \u0026lt;profiles\u0026gt;:\n\u0026lt;profiles\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;ossrh\u0026lt;/id\u0026gt; \u0026lt;activation\u0026gt; \u0026lt;activeByDefault\u0026gt;true\u0026lt;/activeByDefault\u0026gt; \u0026lt;/activation\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;gpg.executable\u0026gt;gpg\u0026lt;/gpg.executable\u0026gt; \u0026lt;gpg.passphrase\u0026gt;passphrase-you-used-when-created-gpg-key\u0026lt;/gpg.passphrase\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;/profiles\u0026gt;  It's ok to have your passphrase set here as this is your user-specific configuration file. If you don't want to specify that, however, there will be an option for you to provide that passphrase every time you publish.\nConfiguring Maven to know the credentials for Sonatype servers You'll provide the log-in credentials in the same settings.xml maven file in ~/.m2 directory. If you don't want to provide the actual username and password, log in to your account at https://oss.sonatype.org . In the top right corner click Log-In, then click your username and select Profile. On the new screen there's a dropdown with two choices: Summary and User Token. Select the user token, it will give you the info. In the settings.xml file add:\n\u0026lt;servers\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;ossrh\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;user-name-token\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;password-for-token\u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;/servers\u0026gt;  Satisfying requirements to pass all checks upon submission to Sonatype There's a lot of meta-info required to satisfy all the requirements. As you will be using the same groupId for all your artifacts, it's easier to put all the extra information to a parent POM. You can find an example parent project here: https://github.com/chhh/sonatype-ossrh-parent . This project consists only of the POM file, specifying the credentials, basic info and publishing locations. It adds some to the release target as well.\nIt's ok to just clone that repo and change the information to what you like. You will set this POM as the \u0026lt;parent\u0026gt; of the projects you wish to publish to Central. As it will be the parent POM, anyone who will want to build your artifacts will need to have that POM, so the first thing is to publish this project to Central by itself.\nPublishing parent POM project to Central We'll be using maven-release plugin. Make sure that:\n You have SCM information configured. In this parent POM you set the version to something like 0.1-SNAPSHOT.  The release plugin will use that information to create the build. It will remove the SNAPSHOT part, build the project, create a new tag in SCM, push everything to remote, bump up the version in POM and re-add SNAPSHOT back to it. Now execute: mvn release:prepare mvn release:perform If you encounter any problems with release:perform you can always do mvn release:rollback to undo any changes done by release:prepare.\nPublishing the project to Central In your actual project set the parent:\n\u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;com.github.chhh\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;sonatype-ossrh-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.1\u0026lt;/version\u0026gt; \u0026lt;relativePath\u0026gt;../sonatype-ossrh\u0026lt;/relativePath\u0026gt; \u0026lt;/parent\u0026gt;  Notice how we used relativePath to give maven a hint at where to search for this POM. The parent project was resiging in a sibling directory next to the project directory in this case. Otherwise the POM would have to be in the parent directory of your project.\n","date":1485331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485331200,"objectID":"4b9034b43daef07ed1258d8cf8abd7ed","permalink":"https://dmtavt.com/post/2017-01-25-publishing-to-maven-central/","publishdate":"2017-01-25T00:00:00-08:00","relpermalink":"/post/2017-01-25-publishing-to-maven-central/","section":"post","summary":"How to push your own libraries to Maven Central repository to make them easily available to everyone in the world.\nJCenter will also copy the libraries from Central, so they will be available from there as well.\n","tags":["maven","sonatype","manual"],"title":"Publishing java libraries to Maven Central, the manual","type":"post"},{"authors":["Andy T Kong","Felipe V Leprevost","Dmitry M Avtonomov","Dattatreya Mellacheruvu","Alexey I Nesvizhskii"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"0256d9aafbe9ec06b480f8fd476bee9b","permalink":"https://dmtavt.com/publication/kong-2017-msfragger/","publishdate":"2020-01-26T23:59:57.840876Z","relpermalink":"/publication/kong-2017-msfragger/","section":"publication","summary":"","tags":null,"title":"MSFragger: ultrafast and comprehensive peptide identification in mass spectrometry--based proteomics","type":"publication"},{"authors":null,"categories":null,"content":"BatMass is a mass-spectrometry data visualization tool, with the main focus on being fast and interactive while providing comprehensive visualizations without any parameter tweaking. It is written in Java and built on top of the NetBeans Platform .\nDemo Video Here's a short demo video of BatMass in action.\n  Some Examples The layout of windows is free and customizable by dragging. Compare multiple experiments at once. The bottom-middle run in this figure is a blank (no sample was injected), while the other 5 were runs with some sample. Amazing how much stuff comes from the background. And here are the same runs but zoomed in to a small region of m/z and retentino time. Look at the color-marked regions. Let's just accept that zero-values are a thing, there is no need to try extracting noise to do gap-filling in data. Features   Support for the open standard mzML and mzXML mass spectrometry data types. We are hoping to bring native vendor format support as well.\n  Viewer synchronization. Link any number of viewers and zooming/panning will be synchronized across them. If you're viewing MS1 data in one view and MS2 data in the other the retention time is synchronized, while m/z is not. Open a detected LC/MS feature table or a peptide identification table, a double click on the row will open the corresponding spectrum, or bring you to the corresponding location in a 2D Map viewer.\n  Data access layer. For the Java developers out there, the highly optimized mzML/mzXML parsers can be used in any standalone Java program as a simple jar dependency. Parsing has been manually tuned to produce few garbage objects, thus minimizing time spent in GC (Garbage Collection), the speed is comparable to or better than in C/C++ implementations. The API for LC/MS data files gives access to most of the features supported by mzML/mzXML standards.\n  Referencing the work If you use BatMass for your research or work, please cite the following paper:\n Avtonomov D.M. et al: J. Proteome Res. June 16, 2016. DOI: 10.1021/acs.jproteome.6b00021. \nContacts The author and maintainer of the project\nDmitry Avtonomov, Ph.D.\nUniversity of Michigan, Ann Arbor\nEmail: {{ config.params.name }}\nGeneral inquiries\nAlexey Nesvizhskii, Ph.D.\nUniversity of Michigan, Ann Arbor\nEmail: nesvi@umich.edu\nhttp://www.nesvilab.org\nPlease use the bug tracker to ask questions, submit feature requests and bug reports.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"1b70d7aadedcd8d8d69f13b2176c580e","permalink":"https://dmtavt.com/project/batmass/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/batmass/","section":"project","summary":"A framework for LC-MS data visualization and exploration. View spectra, extracted ion chromatograms and 2D ion maps of LC-MS experiments. Synchronized viewers.","tags":["mass-spectrometry","data-viz","lc-ms","gui"],"title":"BatMass: mass-spec data visualization","type":"project"},{"authors":null,"categories":null,"content":"Ultra-fast proteomics search engine. Regular searches - when the precursor mass is limited to a narrow window of several ppm - are done within a few seconds. An \u0026ldquo;open-search\u0026rdquo; (with large precursor mass range tolerance, e.g. +/-500 Da) for a 2 hour LC gradient bottom-up proteomics experiment can be done in a minute or so on a laptop.\n","date":1461628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461628800,"objectID":"2c21c3a7a00dc7e3600b589e86de0291","permalink":"https://dmtavt.com/project/msfragger/","publishdate":"2016-04-26T00:00:00Z","relpermalink":"/project/msfragger/","section":"project","summary":"Ultra-fast proteomics search engine. Regular searches - when the precursor mass is limited to a narrow window of several ppm - are done within a few seconds. An \"open-search\" (with large precursor mass range tolerance, e.g. +/-500 Da) for a 2 hour LC gradient bottom-up proteomics experiment can be done in a minute or so on a laptop.","tags":["mass-spectrometry","proteomics","search-engine"],"title":"MSFragger: Ultra-fast proteomics search engine","type":"project"},{"authors":null,"categories":null,"content":"A fast Java library for mass spectrometry data files access (mzML, mzXML, pepXML, protXML, mzId, mzIdentML, etc). Writers for some formats are also available. Available on Maven Central .\n","date":1461628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461628800,"objectID":"d992254f3c75ba7cdb0ef0e0f754704f","permalink":"https://dmtavt.com/project/msftbx/","publishdate":"2016-04-26T00:00:00Z","relpermalink":"/project/msftbx/","section":"project","summary":"A fast Java library for mass spectrometry data files access (mzML, mzXML, pepXML, protXML, mzId, mzIdentML, etc). Writers for some formats are also available. Available on [Maven Central](http://search.maven.org/#search%7Cga%7C1%7Ca%3A%22msftbx%22).","tags":["mass-spectrometry","library","proteomics","lc-ms"],"title":"MSFTBX: MS File Toolbox","type":"project"},{"authors":null,"categories":null,"content":"FragPipe:   MSFragger   Philosopher   Crystal-C   PTM-Shepherd   FragPipe is a Java Graphical User Interface (GUI) for a suite of computational tools enabling comprehensive analysis of mass spectrometry-based proteomics data. It is powered by MSFragger - an ultrafast proteomic search engine suitable for both conventional and \u0026ldquo;open\u0026rdquo; (wide precursor mass tolerance) peptide identification. FragPipe also includes the Philosopher toolkit for downstream post-processing of MSFragger search results (PeptideProphet, iProphet, ProteinProphet), FDR filtering, label-free quantification, and multi-experiment summary report generation. Crystal-C and PTM-Shepherd are included to aid interpretation of open search results. Also included in FragPipe binary are SpectraST-based spectral library building module, and DIA-Umpire SE module for direct analysis of data independent acquisition (DIA) data.\n    ","date":1461542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461542400,"objectID":"718016f416acd3f50dfbac8b8983f5c5","permalink":"https://dmtavt.com/project/fragpipe/","publishdate":"2016-04-25T00:00:00Z","relpermalink":"/project/fragpipe/","section":"project","summary":"A cross-platform pipeline for comprehensive analysis of LC/MS proteomics data. Graphical User Interface (GUI) for running MSFragger (search engine), Philosopher (peptide and protein validation, reports), qunatitation tools, spectral library building and more.","tags":["mass-spectrometry","lc-ms","proteomics","gui"],"title":"FragPipe: Proteomics 0-60 in 5 seconds","type":"project"},{"authors":null,"categories":null,"content":"A suite of tools for extracting signals from Ion Mobility (IM) data (IM-MS, LC-IM-MS) and performing isotopic envelopes detection.\n","date":1461542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461542400,"objectID":"216778e3ae4cffb93b28d3c6a32142fe","permalink":"https://dmtavt.com/project/imtbx/","publishdate":"2016-04-25T00:00:00Z","relpermalink":"/project/imtbx/","section":"project","summary":"A suite of tools for extracting signals from Ion Mobility (IM) data (IM-MS, LC-IM-MS) and performing isotopic envelopes detection.","tags":["mass-spectrometry","ion-mobility","data-viz","lc-ms","library","data-processing","gui"],"title":"IMTBX+Grppr: Ion Mobility Toolbox + Isotopic Grouping","type":"project"},{"authors":null,"categories":null,"content":"       --       DeltaMass will plot mass-differences from a proteomic \u0026ldquo;Open Search\u0026rdquo;. An \u0026ldquo;Open Search\u0026rdquo; is any search of fragmentation spectra (MS2) against a protein database when the precursor mass tolerance was set to a high enough value. E.g. not the usual 5-10-20 ppm, but instead 50-100-200 Da. The result of such a search is quite different from the regular one, it might contain a lot of spurious hits, but a lot of hits will still correspond to reasonable matches.\nThe differences in mass between the observed value for a precursor and the theoretical peptide mass from a database might be many Daltons. Nevertheless, you'll still see lots of similar mass differences, which should correspond to Post Translational Modifications (PTMs) or chemical modifications or other artifacts in the data.\nAfter performing an Open Search you might want to identify the differences to maybe include them into the variable modifications list of your search engine settings. You might also discover unexpected artifacts, like chemical derivatives of peptides occurring because of your sample preparation protocols etc.\nThis is where this program comes into play.\nReferencing work / Citing If you have used this software in your work or research, please cite:\n   Manuscript   Dmitry M. Avtonomov, Andy Kong, and Alexey I. Nesvizhskii, \"DeltaMass: Automated Detection and Visualization of Mass Shifts in Proteomic Open-Search Results\". J. Proteome Res., 2019, 18 (2), pp 715720     DOI   10.1021/acs.jproteome.8b00728     BibTex    @article{Avtonomov2019, author = {Avtonomov, Dmitry M. and Kong, Andy and Nesvizhskii, Alexey I.}, title = {DeltaMass: Automated Detection and Visualization of Mass Shifts in Proteomic Open-Search Results}, journal = {J. Proteome Res.}, publisher = {American Chemical Society}, year = {2019}, volume = {18}, number = {2}, pages = {715--720}, url = {https://doi.org/10.1021/acs.jproteome.8b00728}, doi = {https://doi.org/10.1021/acs.jproteome.8b00728} }     ","date":1461456000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461456000,"objectID":"dd85670b63198fab1ce638b46f78fb80","permalink":"https://dmtavt.com/project/deltamass/","publishdate":"2016-04-24T00:00:00Z","relpermalink":"/project/deltamass/","section":"project","summary":"View and interrogate open search proteomics data.","tags":["mass-spectrometry","data-viz","proteomics","gui"],"title":"DeltaMass: view and interrogate open search proteomics data","type":"project"},{"authors":null,"categories":null,"content":" DIA-Umpire - Data Independent Acquisition (DIA) data processing tool that doens't require a spectral library. Detects features in MS1 and MS2, correlates precursors to fragments based on elution profile similarity and compiles pseudo-MS/MS spectra from these groups, which can later be searched with standard proteomic search tools.\n Efficient calculation of isotopic fine structure up to predefined aggregated probability.  ","date":1461369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461369600,"objectID":"bda1fb20fbabd5e58229df33905a1f29","permalink":"https://dmtavt.com/project/dia-umpire/","publishdate":"2016-04-23T00:00:00Z","relpermalink":"/project/dia-umpire/","section":"project","summary":"Data Independent Acquisition (DIA) data processing tool that doens't require a spectral library. Detects features in MS1 and MS2, correlates precursors to fragments based on elution profile and compiles pseudo-MS/MS spectra which can be searched with standard proteomic search tools.","tags":["mass-spectrometry","proteomics","dia","raw-processing"],"title":"DIA-Umpire","type":"project"},{"authors":["Dmitry M Avtonomov","Alexander Raskind","Alexey I Nesvizhskii"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"1ad57fbaff4cb5dc4eb55bc9542c79a8","permalink":"https://dmtavt.com/publication/avtonomov-2016-batmass/","publishdate":"2020-01-26T23:59:57.839874Z","relpermalink":"/publication/avtonomov-2016-batmass/","section":"publication","summary":"","tags":null,"title":"BatMass: a Java software platform for LC--MS data visualization in proteomics and metabolomics","type":"publication"},{"authors":["Chih-Chiang Tsou","Dmitry Avtonomov","Brett Larsen","Monika Tucholska","Hyungwon Choi","Anne-Claude Gingras","Alexey I Nesvizhskii"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"2620ca848a4c16796ee4e633f9693595","permalink":"https://dmtavt.com/publication/tsou-2015-dia/","publishdate":"2020-01-26T23:59:57.833878Z","relpermalink":"/publication/tsou-2015-dia/","section":"publication","summary":"","tags":null,"title":"DIA-Umpire: comprehensive computational framework for data-independent acquisition proteomics","type":"publication"},{"authors":["Damian Fermin","Dmitry Avtonomov","Hyungwon Choi","Alexey I Nesvizhskii"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"d1db9cc5ca3d33133b53919c0fc06b17","permalink":"https://dmtavt.com/publication/fermin-2015-luciphor-2/","publishdate":"2020-01-26T23:59:57.83487Z","relpermalink":"/publication/fermin-2015-luciphor-2/","section":"publication","summary":"","tags":null,"title":"LuciPHOr2: site localization of generic post-translational modifications from tandem mass spectrometry data","type":"publication"},{"authors":["DM Avtonomov","IA Agron","AS Kononikhin","IA Popov","EN Nikolaev"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"e82d4e2fb073e4d9bfa5e8c474fe4d9e","permalink":"https://dmtavt.com/publication/avtonomov-2011-new/","publishdate":"2020-01-26T23:59:57.838874Z","relpermalink":"/publication/avtonomov-2011-new/","section":"publication","summary":"","tags":null,"title":"A new method for normalization of the peptide retention times in chromatographic/mass-spectrometric experiments","type":"publication"},{"authors":["DM Avtonomov","IA Agron","AS Kononikhin","IA Popov","EN Nikolaev"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"2e8db814e1f95d2268cae72e8d17d012","permalink":"https://dmtavt.com/publication/avtonomov-2011-data/","publishdate":"2020-01-26T23:59:57.837873Z","relpermalink":"/publication/avtonomov-2011-data/","section":"publication","summary":"","tags":null,"title":"Data filtration for more robust alignment of chromatograms of complex peptide mixtures","type":"publication"},{"authors":["IA Agron","DM Avtonomov","AS Kononikhin","IA Popov","SA Moshkovskii","EN Nikolaev"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"c3bcbea17e59e7d286ebdb19a9d38121","permalink":"https://dmtavt.com/publication/agron-2010-accurate/","publishdate":"2020-01-26T23:59:57.835871Z","relpermalink":"/publication/agron-2010-accurate/","section":"publication","summary":"","tags":null,"title":"Accurate mass tag retention time database for urine proteome analysis by chromatography-mass spectrometry","type":"publication"},{"authors":["IA Agron","DM Avtonomov","AS Kononikhin","IA Popov","SA Moshkovsky","EN Nikolaev"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"000a5938d990356eff58ad531947abc6","permalink":"https://dmtavt.com/publication/agron-2010-database/","publishdate":"2020-01-26T23:59:57.842877Z","relpermalink":"/publication/agron-2010-database/","section":"publication","summary":"","tags":null,"title":"Database of accurate mass-timestamp for gas chromatography-mass spectrometry analysis of urine proteome","type":"publication"},{"authors":null,"categories":null,"content":"Designs!! Text!\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e496d665e58251658152d320f71947da","permalink":"https://dmtavt.com/designs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/designs/","section":"","summary":"Designs!! Text!","tags":null,"title":"","type":"widget_page"}]